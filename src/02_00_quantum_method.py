# -*- coding: utf-8 -*-
"""01 - QML in SCM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dRNu559A47ny_07WldCwJpkVvgD8H707

# Libraries
"""

# !pip install tensorflow-addons
# !pip3 install kagglehub

# #Installing Qiskit Packages
# !pip install qiskit
# !pip install qiskit_machine_learning
# !pip install qiskit_algorithms
# !pip install qiskit_ibm_runtime

from pathlib import Path
#import tensorflow as tf
import numpy as np
import pandas as pd
import re
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import time
import requests
import seaborn as sns

#Importing Libraries
from sklearn.svm import SVC

from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import adjusted_rand_score

#from qiskit.utils import algorithm_globals
from qiskit.circuit.library import PauliFeatureMap, ZZFeatureMap
from qiskit_algorithms.state_fidelities import ComputeUncompute
from qiskit_machine_learning.kernels import FidelityQuantumKernel

from qiskit_ibm_runtime import QiskitRuntimeService
from qiskit_machine_learning.algorithms.classifiers import QSVC
from qiskit.circuit import QuantumCircuit
from qiskit import transpile
from qiskit.circuit.library import RealAmplitudes
from qiskit_algorithms.optimizers import COBYLA
from qiskit.primitives import BaseSampler

from sklearn.cluster import SpectralClustering
from sklearn.metrics import normalized_mutual_info_score

from qiskit_ibm_runtime import QiskitRuntimeService, SamplerV2 as Sampler

from sklearn.pipeline import make_pipeline
from qiskit_machine_learning.state_fidelities import ComputeUncompute
from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes
from qiskit_machine_learning.kernels import FidelityQuantumKernel
from qiskit import transpile
from sklearn.svm import SVC
import pandas as pd
import time


import pandas as pd
import os
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

np.random.seed(42)
#algorithm_globals.random_seed = 123

"""# Dataset

[ClaMp](https://www.kaggle.com/code/ssmohanty/dimensionality-reduction-techniques)
"""

#import kagglehub

## Download latest version
#path = kagglehub.dataset_download("saurabhshahane/classification-of-malwares")

#print("Path to dataset files:", path)

import pandas as pd
import os

# Define the dataset path
dataset_path = "/home/ats852/.cache/kagglehub/datasets/saurabhshahane/classification-of-malwares/versions/1"

# List files in the directory to find the CSV file
files = os.listdir(dataset_path)
csv_files = [f for f in files if f.endswith('.csv')]

# Load the first CSV file (assuming there's only one)
if csv_files:
    df = pd.read_csv(os.path.join(dataset_path, csv_files[0]))
    print("CSV found and send to df")  # Display first few rows
else:
    print("No CSV file found in the dataset directory.")

df.head()

target = 'class'

import pandas as pd

def create_balanced_sample(df, target_column='class', num_samples=1000):
    # Ensure that num_samples is even to allow 50/50 split
    if num_samples % 2 != 0:
        raise ValueError("Number of samples must be even to ensure 50% distribution of labels.")

    # Split the data into two groups: one for each class
    class_0 = df[df[target_column] == 0]
    class_1 = df[df[target_column] == 1]

    # Find the minimum number of samples between the two classes to avoid imbalance issues
    min_class_size = min(len(class_0), len(class_1))

    if min_class_size * 2 < num_samples:
        raise ValueError(f"Not enough data to create a balanced dataset of {num_samples} samples.")

    # Sample from each class to ensure 50/50 split
    half_samples = num_samples // 2
    class_0_sample = class_0.sample(n=half_samples)
    class_1_sample = class_1.sample(n=half_samples)

    # Concatenate the two samples to form the balanced dataset
    df_n = pd.concat([class_0_sample, class_1_sample]).sample(frac=1).reset_index(drop=True)  # Shuffle and reset index

    return df_n

# Usage
y = df[target]
X = df.drop(columns=[target])

# Create balanced sample
df_n = create_balanced_sample(df)
print(df_n.shape)

print(df.isna().sum())

#df.dropna(axis=1, inplace=True)
df_clean = df.dropna(axis=1)
#print(df.isna().sum())

y = df_clean[target]
X = df_clean.drop(columns=[target])


class ClaMPDataset(): # 4 features -> most corelated atributs
  def __init__(self, target, cut = 0):
    self.target = target
    self.X, self.y, self.correlation_matrix = self.read_csv(cut)
    #print("X:", self.X.shape)
    self.list_atributs = self.list_atributs_corralation()

  def list_atributs_corralation(self):
    # Unstack the correlation matrix to get pair-wise correlations as a series
    correlation_series = self.correlation_matrix.unstack()
    # Convert the series to a DataFrame for better manipulation
    correlation_df = pd.DataFrame(correlation_series, columns=['correlation']).reset_index()
    # Rename the columns for clarity
    correlation_df.columns = ['attribute_1', 'attribute_2', 'correlation']
    # Filter out self-correlations (where correlation == 1)
    correlation_df = correlation_df[correlation_df['attribute_1'] != correlation_df['attribute_2']]
    # Sort the correlation values from highest to lowest
    sorted_correlation_df = correlation_df.sort_values(by='correlation', ascending=False)
    # List to store the final results without repeated attributes
    top_correlations = []
    list_atributs = []
    used_attributes = set()
    for _, row in sorted_correlation_df.iterrows():
        if len(top_correlations) >= 8:
            break
        attr1, attr2, corr_value = row['attribute_1'], row['attribute_2'], row['correlation']
        if attr1 not in used_attributes and attr2 not in used_attributes:
            top_correlations.append((attr1, attr2, corr_value))
            list_atributs.append(attr1)
            list_atributs.append(attr2)
            used_attributes.add(attr1)
            used_attributes.add(attr2)
    return list_atributs

  def read_csv(self, cut):
    # Define the dataset path
    dataset_path = "/home/ats852/.cache/kagglehub/datasets/saurabhshahane/classification-of-malwares/versions/1"
    # List files in the directory to find the CSV file
    files = os.listdir(dataset_path)
    csv_files = [f for f in files if f.endswith('.csv')]

    # Load the first CSV file (assuming there's only one)
    if csv_files:
        df = pd.read_csv(os.path.join(dataset_path, csv_files[0]))
        #print("CSV found and send to df")  # Display first few rows
    else:
        print("No CSV file found in the dataset directory.")
    #df.dropna(axis=1, inplace=True)
    df = df.dropna(axis=1)
    df = self.create_cut(df, cut)
    y = df[self.target]
    X = df.drop(columns=[self.target])
    X = X.apply(pd.to_numeric,errors='coerce')
    correlation_matrix = X.corr()
    return X,y,correlation_matrix

  def create_cut(self, df, cut=0):
    if cut == 0:
        return df

    # Select the rows where Class is 0 and 1
    class_0 = df[df[self.target] == 0]
    class_1 = df[df[self.target] == 1]

    # Select based on the condition of cut % 2 == 0 or cut % 2 == 1
    if cut % 2 == 0:
        df_n = pd.concat([class_0[:cut//2], class_1[:cut//2]])
    else:
        df_n = pd.concat([class_0[:(cut//2)+1], class_1[:cut//2]])

    return df_n

  def plot_correlation_matrix(self):
    # Plot the correlation matrix using seaborn
    plt.figure(figsize=(15, 8))
    sns.heatmap(self.correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
    plt.title('Correlation Matrix')
    plt.show()


  def stratified_ordered_split(self, X_dim, test_size=0.2):
        # Ensure index alignment
        X_class_0 = X_dim.loc[self.y[self.y == 0].index]
        X_class_1 = X_dim.loc[self.y[self.y == 1].index]
        y_class_0 = self.y[self.y == 0]
        y_class_1 = self.y[self.y == 1]

        # Split each class separately
        # X_train_0, X_test_0, y_train_0, y_test_0 = train_test_split(
        #     X_class_0, y_class_0, test_size=test_size, shuffle=False
        # )
        # X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(
        #     X_class_1, y_class_1, test_size=test_size, shuffle=False
        # )
        X_train_0, X_test_0, y_train_0, y_test_0 = X_class_0, y_class_0, X_class_0, y_class_0
        X_train_1, X_test_1, y_train_1, y_test_1 = X_class_1, y_class_1, X_class_1, y_class_1

        # Concatenate to maintain order
        X_train = np.vstack((X_train_0, X_train_1))
        X_test = np.vstack((X_test_0, X_test_1))
        y_train = np.hstack((y_train_0, y_train_1))
        y_test = np.hstack((y_test_0, y_test_1))

        return X_train, X_test, y_train, y_test    

  def dataset(self, dimension):
    list_atributs = self.list_atributs
    X_dim = self.X[list_atributs[:dimension]]
    X_train, X_test, y_train, y_test = self.stratified_ordered_split(X_dim, test_size=0.2)
    return X_train, X_test, y_train, y_test

class ClaMPDatasetGPT(): # 4 features -> most and least correlated atributs
    def __init__(self, target, cut=0):
        self.target = target
        self.X, self.y, self.correlation_matrix = self.read_csv(cut)
        self.list_atributs = self.list_atributs_correlation()

    def list_atributs_correlation(self):
        # Calculate the correlation of each feature with the target variable
        target_correlation = self.X.corrwith(self.y).abs()

        # Sort the features based on their absolute correlation with the target
        sorted_correlation = target_correlation.sort_values(ascending=False)

        # Select the top n features with the highest correlation
        top_n = 15  # You can adjust this number as needed
        top_correlated = sorted_correlation.head(top_n).index.tolist()

        # Select the top n features with the lowest correlation
        bottom_correlated = sorted_correlation.tail(top_n).index.tolist()

        # Combine the lists
        list_atributs = top_correlated + bottom_correlated

        return list_atributs

    def read_csv(self, cut):
        # Define the dataset path
        dataset_path = "/home/ats852/.cache/kagglehub/datasets/saurabhshahane/classification-of-malwares/versions/1"

        # List files in the directory to find the CSV file
        files = os.listdir(dataset_path)
        csv_files = [f for f in files if f.endswith('.csv')]

        # Load the first CSV file (assuming there's only one)
        if csv_files:
            df = pd.read_csv(os.path.join(dataset_path, csv_files[0]))
            #print("CSV found and send to df")  # Display first few rows
        else:
            print("No CSV file found in the dataset directory.")

        # Drop columns with NaN values
        df = df.dropna(axis=1)
        # Apply the cut if specified
        df = self.create_cut(df, cut)

        # Separate features and target
        y = df[self.target]
        X = df.drop(columns=[self.target])
        X = X.apply(pd.to_numeric, errors='coerce')
        #X = X.dropna()
        # Calculate the correlation matrix
        correlation_matrix = X.corr()

        return X, y, correlation_matrix

    def create_cut(self, df, cut=0):
        if cut == 0:
            return df

        # Select the rows where Class is 0 and 1
        class_0 = df[df[self.target] == 0]
        class_1 = df[df[self.target] == 1]

        # Select based on the condition of cut % 2 == 0 or cut % 2 == 1
        if cut % 2 == 0:
            df_n = pd.concat([class_0[:cut//2], class_1[:cut//2]])
        else:
            df_n = pd.concat([class_0[:(cut//2)+1], class_1[:cut//2]])
        
        return df_n

    def plot_correlation_matrix(self):
        # Plot the correlation matrix using seaborn
        plt.figure(figsize=(15, 8))
        sns.heatmap(self.correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
        plt.title('Correlation Matrix')
        plt.show()

    def stratified_ordered_split(self, X_dim, test_size=0.2):
        # Ensure index alignment
        X_class_0 = X_dim.loc[self.y[self.y == 0].index]
        X_class_1 = X_dim.loc[self.y[self.y == 1].index]
        y_class_0 = self.y[self.y == 0]
        y_class_1 = self.y[self.y == 1]

        # Split each class separately
        # X_train_0, X_test_0, y_train_0, y_test_0 = train_test_split(
        #     X_class_0, y_class_0, test_size=test_size, shuffle=False
        # )
        # X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(
        #     X_class_1, y_class_1, test_size=test_size, shuffle=False
        # )
        X_train_0, X_test_0, y_train_0, y_test_0 = X_class_0, y_class_0, X_class_0, y_class_0
        X_train_1, X_test_1, y_train_1, y_test_1 = X_class_1, y_class_1, X_class_1, y_class_1

        # Concatenate to maintain order
        X_train = np.vstack((X_train_0, X_train_1))
        X_test = np.vstack((X_test_0, X_test_1))
        y_train = np.hstack((y_train_0, y_train_1))
        y_test = np.hstack((y_test_0, y_test_1))

        return X_train, X_test, y_train, y_test

    def dataset(self, dimension):
        list_atributs = self.list_atributs
        X_dim = self.X[list_atributs[:dimension]]
        X_train, X_test, y_train, y_test = self.stratified_ordered_split(X_dim, test_size=0.2)
        return X_train, X_test, y_train, y_test

"""# Quantum Backends

"""

def backends(print_backend=False):
  # Use this command if you didn't save your credentials:
  service = QiskitRuntimeService(channel="ibm_quantum", token="482cfbd6dec57e562dd79640807643b60b7388db6a0ff63e30b4bd92d4d55601a902c6f005413b8a69f6677daf23c82fa14626594ab3656d9c8c0e9229f920de")
  # Load saved credentials
  all = service.backends()
  quantum_backend = ""
  q_hardwares = []
  for i in range(0,len(all)):
    quantum_backend = str(all[i])
    if "<IBMBackend('ibm" in quantum_backend:
      quantum_backend = re.search(r"'(.*?)'", quantum_backend).group(1)  # Extract the text within single quotes
      q_hardwares.append(quantum_backend)
      if (print_backend==True):
        print("Quantum backend:", quantum_backend)
      #break
  return q_hardwares, service

"""## Metrics of Evaluation

We should ahve metrics of evaluation to the Quantum Machine Learning and also the Quantum Hardware with the [backend atributs](https://docs.quantum.ibm.com/api/qiskit-ibm-runtime/qiskit_ibm_runtime.IBMBackend). So with this atribust we will be able to characterize the quantum computers.
"""

def get_confusion_matrix_elements(test_labels, predictions):
    TP = np.sum((test_labels == 1) & (predictions == 1))
    TN = np.sum((test_labels == 0) & (predictions == 0))
    FP = np.sum((test_labels == 0) & (predictions == 1))
    FN = np.sum((test_labels == 1) & (predictions == 0))
    return TP, TN, FP, FN

def mean_qubits(backend, property):
  mean = 0
  for i in range(0,backend.num_qubits):
    if(property == 'readout_error'):
      mean += backend.properties().readout_error(i)
    elif(property=='t1'):
      mean += backend.properties().t2(0)
    elif(property=='t2'):
      mean += backend.properties().t2(0)
  mean=mean/backend.num_qubits
  return mean

def json_qiskit(name):
  TOKEN = "482cfbd6dec57e562dd79640807643b60b7388db6a0ff63e30b4bd92d4d55601a902c6f005413b8a69f6677daf23c82fa14626594ab3656d9c8c0e9229f920de"

  response = requests.request(
    "GET",
    "https://api.quantum-computing.ibm.com/runtime/workloads/me",
    headers={
        "Accept": "application/json",
        "Authorization": "Bearer "+TOKEN
    },
    )
  # Parse the JSON response
  usage_seconds = 0
  estimated_running_time_seconds = 0
  data = response.json()
  # Iterate over the 'workloads' list to find the latest backend with the name
  for workload in data['workloads']:
    if workload['backend'] == name:
          usage_seconds = workload.get('usage_seconds', None)
          estimated_running_time_seconds = workload.get('estimated_running_time_seconds', None)
          break
    else:
        #print(f"Backend {name} not found in the workloads.")
        continue
  return usage_seconds, estimated_running_time_seconds

def calculate_h_m_s_ms(end, start):
  # Calculate elapsed time in seconds
  elapsed_time_seconds = end - start
  # Convert elapsed time into hours, minutes, seconds
  hours = int(elapsed_time_seconds // 3600)
  minutes = int((elapsed_time_seconds % 3600) // 60)
  seconds = int(elapsed_time_seconds % 60)
  milliseconds = int((elapsed_time_seconds - int(elapsed_time_seconds)) * 1000)
  return elapsed_time_seconds#[hours, minutes, seconds, milliseconds]

# Define evaluation metric functions
def calculate_metrics(test_labels, predictions):
    TP, TN, FP, FN = get_confusion_matrix_elements(test_labels, predictions)
    accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0
    precision = TP / (TP + FP) if (TP + FP) > 0 else 0
    recall = TP / (TP + FN) if (TP + FN) > 0 else 0 #recall=sensitivity
    sensitivity = recall
    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0
    # calculate sensitivity and specificity
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    return TP, TN, FP, FN, accuracy, precision, sensitivity, specificity, f1_score

def calculate_metrics_hardware(backend):
  #Read the json according to the backend you are using and get the: 'usage_seconds', 'estimated_running_time_seconds'
  usage_seconds, estimated_running_time_seconds = json_qiskit(backend.name)
  mean_readout_error = mean_qubits(backend,'readout_error')
  mean_t1 = mean_qubits(backend,'t1')
  mean_t2 = mean_qubits(backend,'t1')

  return usage_seconds, estimated_running_time_seconds,mean_readout_error, mean_t1, mean_t2


# Create an empty DataFrame with specified columns
df_results = pd.DataFrame({
    'Model': [],
    'TP': [],
    'TN': [],
    'FP': [],
    'FN': [],
    'Accuracy': [],
    'Precision': [],
    'Sensitivity': [],
    'Specificity': [],
    'F1 Score': [],
    'Elapsed Time (s)':[],
    'Usage (s)':[],
    'Estimated Usage (s)': [],
    'Num Qubits': [],
    'Median T1':[],
    'Median T2':[],
    'Median Read Out Error':[]
})
df_results

def inicial_df():
  df_results = pd.DataFrame({
    'Hardware': [],
    'Dimension': [],
    'TP': [],
    'TN': [],
    'FP': [],
    'FN': [],
    'Accuracy': [],
    'Precision': [],
    'Sensitivity': [],
    'Specificity': [],
    'F1 Score': [],
    'Elapsed Time (s)':[],
    'Usage (s)':[],
    'Estimated Usage (s)': [],
    'Num Qubits': [],
    'Median T1':[],
    'Median T2':[],
    'Median Read Out Error':[]
  })
  return df_results

def metrics_of_evaluation(name, dimension, predicted_labels,end,start,test_features, test_labels, backend):
  usage_seconds, estimated_running_time_seconds,mean_readout_error, mean_t1, mean_t2 = calculate_metrics_hardware(backend)
  enlapse_time = end-start
  predictions = predicted_labels#qsvc.predict(test_features)
  TP, TN, FP, FN, accuracy, precision, sensitivity, specificity, f1_score = calculate_metrics(test_labels, predictions)

  # Create a DataFrame to store the results
  df_results = pd.DataFrame({
      'Hardware':name,
      'Dimension':dimension,
      'TP': [TP],
      'TN': [TN],
      'FP': [FP],
      'FN': [FN],
      'Accuracy': [accuracy],
      'Precision': [precision],
      'Sensitivity': [sensitivity],
      'Specificity': [specificity],
      'F1 Score': [f1_score],
      'Elapsed Time (s)': [enlapse_time],
      'Usage (s)':[usage_seconds],
      'Estimated Usage (s)': [estimated_running_time_seconds],
      'Num Qubits': [backend.num_qubits],
      'Median T1':[mean_t1],
      'Median T2':[mean_t2],
      'Median Read Out Error':[mean_readout_error]

  })
  return df_results

def metrics_of_evaluation_classicaly(svc,dimension,end,start,test_features, test_labels):
  predictions = svc.fit_predict(test_features)
  TP, TN, FP, FN, accuracy, precision, sensitivity, specificity, f1_score = calculate_metrics(test_labels, predictions)
  usage_time = end-start
  # Create a DataFrame to store the results
  df_results = pd.DataFrame({
      'Dimension':dimension,
      'TP': [TP],
      'TN': [TN],
      'FP': [FP],
      'FN': [FN],
      'Accuracy': [accuracy],
      'Precision': [precision],
      'Sensitivity': [sensitivity],
      'Specificity': [specificity],
      'F1 Score': [f1_score],
      'Elapsed Time (s)': [usage_time],
      'Usage (s)':[usage_time],


  })
  return df_results

def should_skip_execution(results_file, samples, start_at):
    if os.path.exists(results_file):
        df_existing = pd.read_csv(results_file)
        # If there are existing records with the same sample count and dimensions greater than or equal to start_at, return True
        if ((df_existing["Samples"] == samples) & (df_existing["Dimension"] >= start_at)).any():
            return True
    
    return False

""" ### Quantum Clustering Model Method """

def qsc(name, X_train, y_train, X_test, y_test, dimension, service, n_clusters=2):
    # Hrdware setup
    backend = service.backend(name)
    sampler = Sampler(backend)
    
    # # Quantum Kernel setup
    # num_features = dimension
    # feature_map = ZZFeatureMap(feature_dimension=num_features, reps=2)
    
    # # Transpile quantum kernel for the local backend
    # feature_map_compiled = transpile(feature_map, backend=backend)
    
    # # Quantum Kernel setup
    # number_qubits = 127 if name != 'ibm_torino' else 133
    # quantum_kernel_circuit = QuantumCircuit(number_qubits)
    # quantum_kernel_circuit.append(feature_map_compiled, range(number_qubits))
    
    # # Use QuantumCircuit as part of FidelityQuantumKernel
    # fidelity_quantum_kernel = FidelityQuantumKernel()
    # fidelity_quantum_kernel._quantum_circuit = quantum_kernel_circuit
    # kernel_matrix = fidelity_quantum_kernel.evaluate(x_vec=X_train)

    
    # # Quantum KMeans clustering
    # clustering = SpectralClustering(n_clusters=n_clusters, affinity='precomputed', assign_labels='kmeans')
    # predicted_labels = clustering.fit_predict(feature_map_compiled)
    
    feature_map = ZZFeatureMap(feature_dimension=dimension, reps=2)
    quantum_kernel = FidelityQuantumKernel(feature_map=feature_map)

    # Compute kernel matrix
    kernel_matrix = quantum_kernel.evaluate(x_vec=X_train)

    # Perform clustering
    clustering = SpectralClustering(n_clusters=n_clusters, affinity='precomputed', assign_labels='kmeans')
    predicted_labels = clustering.fit_predict(kernel_matrix)

    return predicted_labels, backend


def main_qsc_update_correlation(computers='all', dataset=0, samples=500, start_at=0, end_at=20):
    if dataset == 1:        
        results_file = "quantum_clustering_method_results_correlation_10_10.csv"
        if should_skip_execution(results_file, samples, end_at):
            return True
        malware = ClaMPDataset(target='class', cut=samples)
    elif dataset == 0:
        results_file = "quantum_clustering_method_results_correlation_10_0.csv"
        if should_skip_execution(results_file, samples, end_at):
            return True
        malware = ClaMPDatasetGPT(target='class', cut=samples)
    else:
        return True

    if computers != 'all':
        _, service = backends()
        q_hardwares = [computers]
    else:
        q_hardwares, service = backends()

    print("QSVM with high correlation features:")
    
    # **Check if the CSV already exists**
    file_exists = os.path.exists(results_file)

    for dimension in range(start_at, end_at):
        if should_skip_execution(results_file, samples, dimension) == False:
            X_train, X_test, y_train, y_test = malware.dataset(dimension)
            print("Shape: ", X_train.shape, X_test.shape, y_train.shape, y_test.shape)

            for q_hardware in q_hardwares:
                start = time.time()
                predicted_labels, backend = qsc(q_hardware, X_train, y_train, X_test, y_test, dimension, service)

                df_model = metrics_of_evaluation(q_hardware, dimension, predicted_labels, time.time(), start, X_train, y_train, backend)
                df_model["Samples"] = samples  # Add sample size info
                # **Append new results while keeping existing content**
                df_model.to_csv(results_file, mode='a', index=False, header=not file_exists)

                # **Ensure the header is written only once**
                file_exists = True  

    return True

""" ### Classical Cluster Model """

def spectral_clustering_rbf(X_train, y_train, X_test, y_test, n_clusters=3):
    clustering = SpectralClustering(n_clusters=n_clusters, affinity='rbf', assign_labels='kmeans')
    clustering.fit(X_train)
    return clustering

# Main function adapted for clustering with SpectralClustering
def main_clustering_correlation(dataset=0, samples=500, start_at=2, end_at=20):
    if dataset == 0:
        results_file = "classical_clustering_results_correlation_10_10.csv"
        if should_skip_execution(results_file, samples, end_at):
            return True
        malware = ClaMPDataset(target='class', cut=samples)
    else:
        results_file = "classical_clustering_results_correlation_10_0.csv"
        if should_skip_execution(results_file, samples, end_at):
            return True
        malware = ClaMPDatasetGPT(target='class', cut=samples)

    file_exists = os.path.exists(results_file)

    for dimension in range(start_at, end_at):
        if not should_skip_execution(results_file, samples, dimension):
            X_train, X_test, y_train, y_test = malware.dataset(dimension)
            print("Shape: ", X_train.shape, X_test.shape, y_train.shape, y_test.shape)

            start = time.time()
            model = spectral_clustering_rbf(X_train, y_train, X_test, y_test, n_clusters=len(set(y_train)))

            df_model = metrics_of_evaluation_classicaly(model, dimension, time.time(), start, X_train, y_train)
            df_model["Samples"] = samples  # Add sample size info

            # **Append ONLY new results to CSV**
            df_model.to_csv(results_file, mode='a', index=False, header=not file_exists)

            # **Update file_exists to True to prevent rewriting the header**
            file_exists = True  

    return True

""" ### Quantum Cluster Model Circuit """

def quantum_clustering_circuit(name, X_train, y_train, X_test, y_test, dimension, service, n_clusters=2, plot_result = 0):
    # Hardware setup
    backend = service.backend(name)
    #sampler = Sampler(backend)

    # Normalize the features into [0, Ï€] for angle encoding
    scaler = MinMaxScaler(feature_range=(0, np.pi))
    X = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Clustering parameters
    n_clusters = 2
    n_epochs = 100
    n_samples = X.shape[0]
    num_features = X.shape[1]

    # Quantum kernel setup
    feature_map = ZZFeatureMap(feature_dimension=num_features, reps=2)
        
    # Transpile quantum kernel for the local backend
    feature_map_compiled = transpile(feature_map, backend=backend)
        
    # Quantum Kernel setup
    number_qubits = 127 if name != 'ibm_torino' else 133
    quantum_kernel_circuit = QuantumCircuit(number_qubits)
    quantum_kernel_circuit.append(feature_map_compiled, range(number_qubits))
        
    # Use QuantumCircuit as part of FidelityQuantumKernel
    fidelity_quantum_kernel = FidelityQuantumKernel()
    fidelity_quantum_kernel._quantum_circuit = quantum_kernel_circuit

    # Initialize random cluster centers
    cluster_centers = X[np.random.choice(n_samples, n_clusters, replace=False)]

    # Main training loop (Quantum Kernel k-means style)
    for epoch in range(n_epochs):
        #print(f"Epoch {epoch+1}/{n_epochs}")
        
        y_pred = np.zeros(n_samples)

        # Assign points to nearest cluster using quantum kernel similarity
        for i, x in enumerate(X):
            similarities = [fidelity_quantum_kernel.evaluate(x.reshape(1, -1), c.reshape(1, -1))[0, 0]
                            for c in cluster_centers]
            y_pred[i] = np.argmax(similarities)

        # Update cluster centers (mean of assigned samples)
        for j in range(n_clusters):
            points_in_cluster = X[y_pred == j]
            if len(points_in_cluster) > 0:
                cluster_centers[j] = np.mean(points_in_cluster, axis=0)

    # Evaluate clustering performance
    print(y_train.shape, y_pred.shape)
    score = adjusted_rand_score(y_train, y_pred)
    print(f"\nFinal Clustering Score (Adjusted Rand Index): {score:.2f}")

    if plot_result == 1:
        # Plot the final clustering
        plt.figure(figsize=(8, 6))
        plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis', alpha=0.6)
        plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], c='red', marker='X', s=100)
        plt.title("Quantum Kernel Clustering of Iris Dataset")
        plt.xlabel("Feature 1")
        plt.ylabel("Feature 2")
        plt.grid(True)
        plt.show()

    return y_pred, backend

def main_qcc_update_correlation(computers='all', dataset=0, samples=500, start_at=0, end_at=20):
    if dataset == 1:        
        results_file = "quantum_clustering_circuit_results_correlation_10_10.csv"
        if should_skip_execution(results_file, samples, end_at):
            return True
        malware = ClaMPDataset(target='class', cut=samples)
    elif dataset == 0:
        results_file = "quantum_clustering_circuit_results_correlation_10_0.csv"
        if should_skip_execution(results_file, samples, end_at):
            return True
        malware = ClaMPDatasetGPT(target='class', cut=samples)
    else:
        return True

    if computers != 'all':
        _, service = backends()
        q_hardwares = [computers]
    else:
        q_hardwares, service = backends()

    print("QSVM with high correlation features:")
    
    # **Check if the CSV already exists**
    file_exists = os.path.exists(results_file)

    for dimension in range(start_at, end_at):
        if should_skip_execution(results_file, samples, dimension) == False:
            X_train, X_test, y_train, y_test = malware.dataset(dimension)
            print("Shape: ", X_train.shape, X_test.shape, y_train.shape, y_test.shape)

            for q_hardware in q_hardwares:
                start = time.time()
                predicted_labels, backend = quantum_clustering_circuit(q_hardware, X_train, y_train, X_test, y_test, dimension, service)

                df_model = metrics_of_evaluation(q_hardware, dimension, predicted_labels, time.time(), start, X_train, y_train, backend)
                df_model["Samples"] = samples  # Add sample size info
                # **Append new results while keeping existing content**
                df_model.to_csv(results_file, mode='a', index=False, header=not file_exists)

                # **Ensure the header is written only once**
                file_exists = True  

    return True

"""### Quantum Clustering Method"""
TOTAL_SAMPLES = X.shape[0]
i = 0
# for i in range(0, 2): # Type of features selection: (High High) vs (High Low)
for j in range(1, 11): # Size of dataset: 10%, 20%, ..., 100%
    samples = int((TOTAL_SAMPLES * j) / 10)   
    df_result_qsvc0 = main_qsc_update_correlation("ibm_brisbane", dataset=i, samples = samples, start_at=2, end_at=11)
print("**********************************************")

"""### Classical Clustering"""
# # for i in range(0, 2): # Type of features selection: (High High) vs (High Low)
# for j in range(1, 11): # Size of dataset: 10%, 20%, ..., 100%
#     samples = int((TOTAL_SAMPLES * j) / 10)  
#     df_result_svc0 = main_clustering_correlation(i, samples = samples, start_at=2, end_at=20) #"ibm_brisbane", start_at=2, end_at=20)
# print("**********************************************")

"""### Quantum Cluster Circuit"""

# # for i in range(0, 2): # Type of features selection: (High High) vs (High Low)
# for j in range(1, 11): # Size of dataset: 10%, 20%, ..., 100%
#     samples = int((TOTAL_SAMPLES * j) / 10)   
#     df_result_qkernel0 = main_qcc_update_correlation(dataset = i, samples = samples, start_at=2, end_at=11)
# print("**********************************************")
